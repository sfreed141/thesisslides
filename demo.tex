\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle,block=fill]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\usepackage{multimedia}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage{algorithm,algpseudocode} % https://tex.stackexchange.com/a/230789

\graphicspath{ {figures/} }

% https://tex.stackexchange.com/a/160827
\newcommand\Wider[2][3em]{%
\makebox[\linewidth][c]{%
  \begin{minipage}{\dimexpr\textwidth+#1\relax}
  \raggedright#2
  \end{minipage}%
  }%
}

\title{Tessellated Voxelization for Global Illumination using Voxel Cone Tracing}
%\subtitle{}
\date{June 2018}
\author{Sam Freed\\\\Advisor: Dr.\ Christian Eckhardt\\Committee Members: Dr.\ Maria Pantoja, Dr.\ Aaron Keen\\}
\institute{California Polytechnic State University, San Luis Obispo}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\AtBeginSection[]
{
	\begin{frame}
    	\frametitle{Outline}
        \tableofcontents[currentsection,hideothersubsections] % TODO show subsections/frames?
	\end{frame}
}

\begin{document}

\frame{\titlepage}


% OVERALL GUIDELINES (from Christian)
% you are _teaching_: try to explain in simple but not condescending terms
% do not make it sound too complicated: no one wants a know-it-all speaker
% mistakes will happen, laugh them off and don't be too hard on yourself
% move around and point things out in images
% don't rush and be careful of presentation fatigue
% pseudocode should be extremely brief (6 lines max); any algorithm should be explainable in ~4 sentences
% BE CONFIDENT


% Go over outlines of presentation. Mention the subsections briefly as appropriate.
\begin{frame}{Outline}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\setbeamertemplate{frametitle continuation}{}

\section{Introduction}

\begin{frame}{Introduction}
% who i am, thanks for coming, blah blah

% what is global illumination and why important
% hopefully after this everyone will have an idea of how to do this themselves :)
% show AO and baked? or at least talk about it and show pictures
% we want FULL GI
  \begin{block}{Computer Graphics}
    How can we model light in a virtual world?

    Physically accurate lighting too complex for real-time---approximate!
  \end{block}
\end{frame}

\begin{frame}{Computer Graphics}
  % here we only have simple lighting. the ambient/indirect light is the difficult part to compute

  % Want to go from this...to _this_
  % \begin{columns}
  %   \begin{column}{0.5\textwidth}
      \begin{figure}
        \includegraphics[width=\textwidth]{gi_off.png}
        :(
      \end{figure}
    \end{frame} \begin{frame}{Motivations}
  %   \end{column}
  %   \begin{column}{0.5\textwidth}
      \begin{figure}
        \includegraphics[width=\textwidth]{gi_on.png}
        :)
      \end{figure}
  %   \end{column}
  % \end{columns}
\end{frame}

\begin{frame}{Real-Time Global Illumination}
  Various approaches to approximating indirect light.

  \begin{description}
    \item[Constant] Fixed fraction of ambient light % as seen in first image
    \item[Partial] Ambient occlusion, shadows, screen space reflections % only model parts of global illumination; often local methods that do not have complete scene information
    \item[Static] Baked lighting, light probes
    \item[Dynamic] Light Propagation Volumes, Voxel Cone Tracing
  \end{description}
\end{frame}

% \begin{frame}{Teaser}
% % video or screenshots, show good lighting + moving objects
% \movie[autostart,showcontrols]{\textcolor{black}{\rule{\textwidth}{0.9\textheight}}}{test.webm}
% \end{frame}

% \begin{frame}{What did I just see?}
% % real time gi of course!
% \end{frame}

\subsection{Motivations}
\begin{frame}{Motivations}
  \begin{enumerate}
    \centering
    \item Real-time global illumination is difficult % lots of computation
    \item Limited reference material % open-source, cross-platform (-apple), no large engine, any documentation or explanation
    \pause
    \item It's cool % the real reason tbh
  \end{enumerate}
\end{frame}

\subsection{Contributions}
\begin{frame}{Contributions}
  \begin{itemize}
    \item Open-source, cross-platform implementation of global illumination using voxel cone tracing
    \item Comparison between rasterization and tessellation based voxelization
    \item Investigation into warped voxels % more on this later, first I'd like to give a crash course in graphics and an overview of other solutions to real time gi
  \end{itemize}
\end{frame}


\section{Background}

\subsection{Computer Graphics}
\begin{frame}{Computer Graphics Primer}
  \begin{block}{Goal}
    Given a virtual description of a scene, render an image.
  \end{block}

  \begin{block}{Big Issues}
    \begin{enumerate}
      \item How do we represent a scene? What information is required?  % geometry (triangles, voxels), materials (colors, properties), lights (type, position, color, direction), etc.
      \item How is a 3D scene represented as a 2D image?  % implies some transformation needed
      \item How do we render---how is the final pixel color computed? % lighting/shading model; this is the focus of this thesis
    \end{enumerate}
  \end{block}
  % rtr objects -> world space -> screen space
  % goal of computer graphics; graphics pipeline; transforms and other common stuff
\end{frame}

\begin{frame}{How do we represent a scene?}
  \setbeamercovered{transparent}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}[<+>]
        \item Geometry: triangles, voxels % triangles inherently 2D->good for surfaces, not so much for volumetric data
        \item Materials: colors and other properties % like roughness, normal maps (kinda)
        \item Lights: positions, colors, etc.
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \only<1>{
        \begin{figure}
          \begin{subfigure}{\textwidth}
            \includegraphics[width=\textwidth]{wireframe.png}
          \end{subfigure}
          \begin{subfigure}{\textwidth}
            \includegraphics[width=\textwidth]{voxels.png}
          \end{subfigure}
        \end{figure}}

      \only<2>{
        \begin{figure}
          \begin{subfigure}{\textwidth}
            \includegraphics[width=\textwidth]{albedo.png}
          \end{subfigure}
          \begin{subfigure}{\textwidth}
            \includegraphics[width=\textwidth]{roughness.png}
          \end{subfigure}
        \end{figure}}

      \only<3>{
        \begin{figure}
          \begin{subfigure}{\textwidth}
            \includegraphics[width=\textwidth]{nogi.png}
          \end{subfigure}
          \begin{subfigure}{\textwidth}
            \includegraphics[width=\textwidth]{gi.png}
          \end{subfigure}
        \end{figure}}
    \end{column}
  \end{columns}
\end{frame}

{\setbeamertemplate{frame footer}{image from Real-Time Rendering} % TODO cite
\begin{frame}{How is a 3D scene represented as a 2D image?}
  Math!

  All coordinates are transformed multiple times before ending up at their appropriate place on the screen.

  \begin{figure}
    \includegraphics[width=\textwidth]{rtr_fig2_1.png}
  \end{figure}

\end{frame}}


\begin{frame}{How is a 3D scene represented as a 2D image?}
  % picture of coordinate systems and give brief explanation, since explain more along with the graphics pipeline
  % showing this so everyone is comfortable when explaining algorithms
  % ...and this is accomplished with <flick> THE GRAPHICS PIPELIIIIIIINE

  \begin{figure}
    \includegraphics[width=\textwidth]{learnopengl_coordinate_systems.png}
  \end{figure}

  \begin{description}[<+| only@+>]
    \item[Local/Object Space] coordinate system relative to a single object
    \item[World Space] a global coordinate system for the virtual world
    \item[View Space] coordinates are with respect to the camera
    \item[Clip Space/NDC] defines a frustum in which objects will be rasterized % those outside the frustum are clipped
    \item[Screen Space] coordinates are the pixel position and a depth value
  \end{description}

  % Also other spaces, like texture space [0, 1], image space (integral coordinates),

\end{frame}

\begin{frame}{How is a 3D scene represented as a 2D image?}
  \begin{center}
    \huge\textbf{The Graphics Pipeline}
  \end{center}
  % picture of pipeline
  % This is the standard sequence of events that render the final image from our scene
  % We'll go over the entire pipeline for sake of completess
  \begin{figure}
    \includegraphics[width=\textwidth]{learnopengl_graphicspipeline.png}
  \end{figure}
\end{frame}

\begin{frame}{The Graphics Pipeline}
  \begin{figure}
    \includegraphics[width=\textwidth]{learnopengl_graphicspipeline.png}
  \end{figure}

  \begin{description}[<+| only@+>]
    \item[Input] A series of vertices containing attributes like position, normal, and texture coordinates.
    \item[Vertex Shading] Executed for each vertex. Usually transforms points to clip space.
    \item[Shape Assembly] Form geometric primitives from vertices (like triangles).
    \item[Geometry Shading] Operate on primitives.
    \item[Rasterization] Generate fragments for pixels covered by primitives.
    \item[Fragment Shading] Executed for each fragment. Usually shading occurs here.
    \item[Testing and Blending] Depth testing and alpha blending.
  \end{description}

  % There are also compute shaders for operating on arbitrary data and tessellation shaders for dynamically subdividing primitives (as we'll see in a bit).
\end{frame}

% \begin{frame}{Compute Shader}
%   % just want to introduce the concept since I'll mention them in implementation
% \end{frame}

% \begin{frame}{Tessellation}
%   % briefly go over triangle tessellation and where it fits in the pipeline. will probably talk about it more in implementation

%   \begin{figure}
%     \includegraphics[height=\textheight]{openglwiki_tesselation.png}
%   \end{figure}
% \end{frame}

\subsection{Lighting}
\begin{frame}{How do we render?}
  % We need a shading model: a way to compute a color from the geometry, lights, and materials we have in the scene for a particular point
  % Makes sense to model this based on the real-world sooo
  \begin{figure}
    \includegraphics[width=\textwidth]{indirectlighting}
  \end{figure}
\end{frame}

\begin{frame}{Light Theory}
  % Lighting algorithms are numerical approximations
  % Note how this is computationally intensive -> need to resort to approximations

  \begin{figure}
    \begin{tikzpicture}
      \draw (2, 0) arc (0:180:2);
      \draw (0, 0) ellipse (2cm and 0.5cm);

      \fill (0, 0) circle (0.05cm) node[below] (x) {$x$};

      \draw[->] (0, 0) -- (30:3cm) node[above right] {$\bm{v}$};
      \draw[->] (0, 0) -- (90:3cm) node[above] {$\bm{n}$};
      \draw[->] (0, 0) -- (120:3cm) node[above left] {$\bm{l_{direct}}$};
      \draw[->] (0, 0) -- (150:3cm) node[above left] {$\bm{l_{indirect}}$};

      \path (30:5cm) coordinate (camera);
      \fill (camera) circle (0.1cm) node[above] {Camera};

      \path (120:5cm) coordinate (light);
      \fill (light) circle (0.1cm) node[above] {Light Source};

      \path (150:5.8) coordinate (bounce);
      \fill (-6, 0) rectangle (-5, 4);
      \path (bounce) circle (0.1cm);
      \draw[dashed, ->] (light) -- (bounce);
      \foreach \angle in {60, 30, 0, -30, -60} {
        \draw[dashed, ->] (bounce) -- +(\angle:0.5cm);
      }
    \end{tikzpicture}
  \end{figure}

  % TODO rendering equation?
\end{frame}

\begin{frame}{Global Illumination}
  Most algorithms follow a few main steps:

  \begin{enumerate}
    \item Construct representation of the scene
    \item Derive indirect lighting information from representation
    \item Collect indirect lighting when rendering
  \end{enumerate}
\end{frame}

\begin{frame}{Spatial Data Structures}
  % quickly go through one slide each (with picture)
  \begin{description}
    \item<1>[Uniform Texture] each cell is the same size  % simple, hw filtering; wasteful
    \item<2>[Cascaded Texture/Clipmap] nested textures with varying cell sizes % there is difference but not important for us; cell sizes are discrete multiplies, still wasteful but better, need to handle edges properly
    \item<3>[Octree] adaptive tree structure which holds values in leaves % sparse, difficult on gpu to build and traverse, no hw filtering
  \end{description}

  \begin{figure}
    % TODO TODO TODO \includegraphics<+>[width=\textwidth]{}
    \only<1>{\begin{tikzpicture}[scale=0.8]
      \draw [black, step=1.0] (-4, -4) grid ++(8, 8);
    \end{tikzpicture}}
    % \includegraphics<+>[width=\textwidth]{geometry_clipmap.jpg}
    \only<2>{\begin{tikzpicture}[scale=0.8]
      \draw [black, step=1.0] (-4, -4) grid ++(8, 8);
      \draw [black, step=0.5] (-2, -2) grid ++(4, 4);
      \draw [black, step=0.25] (-1, -1) grid ++(2, 2);
    \end{tikzpicture}}
    % \includegraphics<3>[width=\textwidth]{octree.png}
    \only<3>{\begin{tikzpicture}[scale=0.8]
      \draw [black, step=4.0] (-4, -4) grid ++(8, 8);
      \draw [black, step=2.0] (0, 0) grid ++(4, 4);
      \draw [black, step=1.0] (2, 2) grid ++(2, 2);
      \draw [black, step=1.0] (2, 0) grid ++(2, 2);
      \draw [black, step=0.5] (2, 1) grid ++(1, 2);
      \draw [black, step=0.25] (2, 1.5) grid ++(1, 1);
    \end{tikzpicture}}
  \end{figure}
\end{frame}

% raytracing and raymarching? maybe one slide

% ask for any questions or clarification here? (after related work too maybe?)

\section{Related Work}

\subsection{VPLs and RSMs}
\begin{frame}{VPLs and RSMs}
  % Just want to introduce the concept of VPLs and RSMs are simple way of doing that
  \begin{description}
    \item[Virtual Point Lights] Treat arbitrary objects or locations as light sources
    \item[Reflective Shadow Maps] Treat each pixel of a shadowmap as a VPL % when rendering project point into shadowmap and gather lighting contributions from VPLs; problem is poor occlusion information and only low frequency lighting
  \end{description}

  \begin{figure}
    \includegraphics<+>[width=\textwidth]{rsm.png}
    \includegraphics<+>[width=\textwidth]{rsm_vpl.png}
  \end{figure}

  % another slide for pros/cons?
\end{frame}

% LPVs and VCT after implementation? doesn't really help as background info
\subsection{Light Propagation Volumes}
\begin{frame}{Light Propagation Volumes}
  % low frequency but fairly stable, no occlusion (limited info from reusing), cascaded
  % iterative process to propagate light throughout scene

  \begin{itemize}
    \item Store VPLs in a voxel grid
    \item Iteratively propagate lighting contribution until stable
  \end{itemize}

  \begin{figure}
    \includegraphics[width=\textwidth]{lpv.png}
  \end{figure}
\end{frame}

\subsection{Voxel Cone Tracing}
\begin{frame}{Voxel Cone Tracing}
  % complete occlusion info, sparse voxel octree, specular reflections

  \begin{itemize}
    \item Sparse voxel octree % occlusion information
    \item High resolution (diffuse and specular lighting)
  \end{itemize}

  \begin{figure}
    \includegraphics[width=\textwidth]{vct.png}
  \end{figure}
\end{frame}

% TODO quick recap here?
% most important bits: voxels represent 3d data well and thus voxelization is important; vpls are core part of most gi algorithms; lpv and vct use regular grid

\begin{frame}{What am I doing?} % this is the research question
  % common theme is constructing some (ideally 3D) representation of a scene
  % we wanted to evaluate an alternate method of voxelization and to see if nonuniform voxel sizes were a promising alternative to existing 3D data structures
  \begin{enumerate}
    \item Comparing two approaches for scene voxelization % voxelization is a core part of the algorithm; tessellated version also not restricted to rasterizer
    \item Investigating continuous voxel sizes % voxel warping; idea is to support bigger scenes and increase resolution near camera without abrupt changes in voxel density
  \end{enumerate}
\end{frame}

\section{Implementation}

\begin{frame}{Overview of Renderer}
  % flowchart or something showing each render pass
  % list of all major resources (3ish voxel textures, shadowmap, meshes, materials)

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{block}{Main Steps}
        \begin{enumerate}
          \item Setup (load scene, create textures, compile shaders)
          \item Create Shadowmap
          \item Voxelize Scene
          \item Inject Radiance
          \item Filter Radiance
          \item Depth Prepass
          \item Shading
        \end{enumerate}
      \end{block}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{block}{Important Data}
        \begin{enumerate}
          \item Scene (meshes, materials, lights)
          \item Camera (position, direction) %, projection + view matrices)
          \item Shadowmap % Framebuffer Object and Texture
          \item Voxel Textures (color + opacity, normals, radiance)
        \end{enumerate}
      \end{block}
    \end{column}
  \end{columns}
\end{frame}

\subsection{Voxelization}
\begin{frame}{Voxelization with Rasterizer}
  Each fragment corresponds to a voxel
  % main idea: rasterized fragments correspond to voxel
  % setup: need projection matrices
  % geometry shader: choose dominant axis and project
  % fragment shader:

  \begin{figure}
    \includegraphics[width=\textwidth]{rasterizedvoxelization}
  \end{figure}
\end{frame}

\begin{frame}{Conservative Rasterization}
  % Of particular importance with the rasterization approach is the need for conservative rasterization. In general, a fragment is only generated for a triangle if that triangle overlaps the center of the fragment (pixel). This can cause undesirable cracks and holes in the voxelization. Conservative rasterization is meant to address this by generating a fragment for even partially overlapped fragments.

  \begin{figure}
    \begin{subfigure}[t]{0.4\textwidth}
      \includegraphics[width=\textwidth]{conservativeraster_off}
      \caption*{Off}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.4\textwidth}
      \includegraphics[width=\textwidth]{conservativeraster_on}
      \caption*{On}
    \end{subfigure}
  \end{figure}
\end{frame}

\begin{frame}{Conservative Rasterization}
  % Here is a comparison between no conservative rasterization and with. We use an MSAA based method (a good compromise between simplicity, quality, and it's cross vendor)

  \Wider{\begin{figure}
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{voxels_off}
      \caption*{Off}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{voxels_msaa}
      \caption*{On}
    \end{subfigure}
  \end{figure}}
\end{frame}

\begin{frame}{Voxelization with Tessellator}
  Idea: Generate a \textit{vertex} for each voxel instead of a fragment

  % Tessellation after vertex shading and before fragment shading
  % rasterizer is not needed (is disabled)

  % Tessellation is a method of programmatically subdividing input primitives (in our case triangles).

  \begin{figure}
    \includegraphics[scale=0.8]{openglwiki_tesselation.png}
  \end{figure}
\end{frame}

\begin{frame}{Determining Tessellation Levels}
  Outer levels determined from respective edge lengths

  Inner level determined from maximum triangle altitude length

  % Here is an illustration of the idea behind tessellated voxelization. By subdividing the edges and inside appropriately we can get a fairly good voxelization. A nice part of this voxelization is we don't need to worry about the dominant axis or conservative rasterization.
  \begin{figure}
    \begin{tikzpicture}[scale=0.8]
      \draw [gray, step=1.0] (-4, -4) grid ++(8, 8);
      \foreach \x/\y in {-3.1/-0.2, -2.23/-0.7, -1.37/-1.2, -0.5/-1.7, 0.25/-0.65, 1.0/0.4, 1.75/1.45, 2.5/2.5, 1.57/2.05, 0.63/1.6, -0.3/1.15, -1.23/0.7, -2.17/0.25, -1.31/-0.28, -0.19/0.26, -0.69/-0.58}{
        \fill[thick, black] (\x, \y) circle (0.1cm);
      }
      \draw [black] (-3.1, -0.2) -- (2.5, 2.5) -- (-0.5, -1.7) -- cycle;
      \draw [black] (-1.31, -0.28) -- (-0.19, 0.26) -- (-0.69, -0.58) -- cycle;
    \end{tikzpicture}
  \end{figure}
\end{frame}

\begin{frame}{Voxelized Scene}
  % Opacity, color, and normals are stored in voxel textures. Since multiple shader invocations can write to the same voxel we need to use atomic operations. Both an atomic max and atomic average are supported.
  \begin{figure}
    \includegraphics[width=\textwidth]{voxels_msaa}
  \end{figure}
\end{frame}

\subsection{Radiance Injection and Filtering}
\begin{frame}{Radiance Injection}
  \begin{itemize}
    \item Create virtual point lights for all geometry hit by the light.
    \item These lights approximate a single-bounce of indirect lighting.
  \end{itemize}

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{lightinjection}
  \end{figure}
\end{frame}

\begin{frame}{Radiance Injection---Shadow Mapping}
  To determine where the virtual point lights should be, we use a \textbf{shadowmap}.

  Render the scene from the \textit{light}'s point of view.

  % Only interested in depth values important for us: along with the light matrix we can reconstruct world positions
  % picture
  % implementation? ortho matrix, projection

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{shadowmap}
  \end{figure}
\end{frame}

\begin{frame}{Radiance Injection---Injecting VPLs}
  For each pixel in the shadowmap, find it's voxel index and insert the corresponding color into the radiance texture.

  Using the light matrix and stored depth value, we compute the point's world space position.

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{radiance_nolighting}
  \end{figure}
\end{frame}

\begin{frame}{Radiance Filtering}
  Radiance texture contains multiple \textbf{levels} (mipmaps). Each level is half the size of the previous. A compute shader performs a 2x2x2 box filter for each level to determine the filtered value.

  \begin{center}
    \begin{tikzpicture}
      \draw[step=1.0, gray] (0, 0) grid (4, 4);
      \draw[step=2.0, black, ultra thick] (0, 0) grid (4, 4);
      \draw[black, ->, ultra thick] (4.25, 2) to (4.75, 2);
      \draw[step=2.0, black, ultra thick, xshift=5.0cm] (0, 0) grid (4, 4);
    \end{tikzpicture}
  \end{center}

  % show 2D box filter
  % show the four images
\end{frame}

\begin{frame}{Radiance Filtering}
  \begin{figure}
    \includegraphics<+>[width=\textwidth]{mipmap0.png}
    \includegraphics<+>[width=\textwidth]{mipmap1.png}
    \includegraphics<+>[width=\textwidth]{mipmap2.png}
    \includegraphics<+>[width=\textwidth]{mipmap3.png}
    \caption*{\only<1>{Mipmap level 0}\only<2>{Mipmap level 1}\only<3>{Mipmap level 2}\only<4>{\\Mipmap level 3}}
  \end{figure}
\end{frame}

\subsection{Final Shading}

% TODO remove?
\begin{frame}{Depth Prepass}
  An optimization to minimize the number of shaded fragments.

  Prefill the depth buffer so lighting (including the relatively expensive voxel cone tracing) will only be performed on the visible fragments of the scene. % remind of depth test and this is only for forward rendering (deferred already does this when creating gbuffer)
\end{frame}

\begin{frame}{Final Shading}
  % outline (direct + diffuse indirect + specular indirect)
  % inputs are lights, material, and fragment (with position, normal, texture coordinates)
  % Indirect lighting is the only interesting (non-standard) one here
  \begin{description}
    \item[Direct Lighting] sum light contributions using Cook-Torrance shading model
    \item[Indirect Lighting] voxel cone tracing
    \item[Post Processing] tone mapping and gamma correction
  \end{description}
\end{frame}

\begin{frame}{Direct Lighting}
  % psuedocode? or just picture of direct light and explain?
  \begin{algorithm}[H]
    \begin{algorithmic}
      \State {color = 0}
      \For {each light in the scene}
        \If {not in shadow}
          \State {color += computeLighting()}
        \EndIf
      \EndFor
    \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame}{Indirect Lighting}
% explain voxel cone tracing
% the idea (raymarching + mipmaps), choosing cone angles + dirs, diffuse indirect, specular indirect, cone tracing itself

% picture of cones again first?

% To approximate indirect lighting (recall that we are approximating a hemispherical surface integral) we use voxel cone tracing. The idea combines raytracing and mipmapping: we are raymarching through our filtered radiance values. The level to sample from is determined by the cone's diameter: at each step we increase the cone's height (also increasing the diameter)

% Here we are determining the indirect light at the point on this yellow sphere here with surface normal n. At each step, we sample from a higher level corresponding to the cone's diameter. We use 5 cones to gather diffuse (low frequency) light and 1 cone for specular lights. The cone tracing also gives us occlusion information, which we use to attenuate the lighting.
  \begin{figure}
    \includegraphics<+>[width=\textwidth]{conetrace1}
    \includegraphics<+>[width=\textwidth]{conetrace2}
    \includegraphics<+>[width=\textwidth]{conetrace3}
    \includegraphics<+>[width=\textwidth]{conetrace4}
    \includegraphics<+>[width=\textwidth]{conetrace5}
  \end{figure}
\end{frame}

\begin{frame}{Indirect Lighting}
  \begin{figure}
    \includegraphics<1>[width=\textwidth]{debugIndirect_noOcclusion}
    \includegraphics<2>[width=\textwidth]{debugReflections}
    \includegraphics<3>[width=\textwidth]{debugOcclusion}
    % TODO combined total indirect lighting
    \caption*{
      \only<1>{Diffuse Indirect (no occlusion)}
      \only<2>{Specular Indirect}
      \only<3>{\\Occlusion} % note this is voxel based ambient occlusion
    }
  \end{figure}
\end{frame}

\subsection{Voxel Warping}
\begin{frame}{Voxel Warping}
  % TODO picture of linear on one side and warped on other with guidelines (just do cubic) and show warp slope and voxels
\end{frame}


\section{Results and Conclusions}

% \begin{frame}{Test Setup}
% \end{frame}

% First, and most important, test was to measure the general performance of the algorithm. We also varied the voxel grid resolution and screen resolution to see how it affected performance. Here are some screenshots of the varying voxel grid dimensions, starting at 64x64x64

\begin{frame}{Performance}
  \begin{figure}
    \includegraphics<1>[width=\textwidth]{results_720_64}
    \includegraphics<2>[width=\textwidth]{results_720_128}
    \includegraphics<3>[width=\textwidth]{results_720_256}
    \caption*{
      \only<1>{$64^3$ voxel grid}
      \only<2>{$128^3$ voxel grid}
      \only<3>{\\$256^3$ voxel grid}
    }
  \end{figure}
  % pictures of different voxel grid dimensions
  % graphs of performance times for each
  % analysis (which render passes were affected by what, etc)
\end{frame}

% And here we have the timing information. Each bar is for a given screen resolution and voxel grid resolution and shows the individual contributions of each render pass. For the typical 60fps goal for real-time (which is about 17ms) you see we meet the 60fps goal for all tested resolutions!
% Then talk about individual contributions (constant, varies with screen, varies with grid, with both)
\begin{frame}{Performance}
  \begin{figure}
    \pgfplotstableread[row sep=\\]{
      Resolution Voxelize Shadowmap Inject Filter Prepass Shading\\
      720p64 0.90  0.69  0.92  0.04  0.20  3.89\\
      720p128 1.13  0.69  0.92  0.10  0.20  4.81\\
      720p256 2.40  0.69  0.93  0.55  0.20  5.32\\
      900p64 0.70  0.69  0.92  0.05  0.25  5.98\\
      900p128 1.12  0.69  0.92  0.10  0.26  6.42\\
      900p256 2.39  0.69  0.93  0.56  0.25  7.10\\
      1080p64 0.72  0.68  0.92  0.04  0.31  8.28\\
      1080p128 1.15  0.68  0.92  0.10  0.31  9.40\\
      1080p256 2.41  0.69  0.93  0.55  0.36  9.75\\
    }\dataset
    \begin{tikzpicture}[scale=0.9]
      \begin{axis}[nodes near coords ybar stacked configuration/.style={},ybar stacked, symbolic x coords={720p64, 720p128, 720p256, 900p64, 900p128, 900p256, 1080p64, 1080p128, 1080p256}, xtick=data, xticklabel style={rotate=45}, legend style={at={(1.05,0.5)}, anchor=west}, xlabel={Screen Resolution x Voxel Grid Size}, ylabel={Time (ms)}, enlarge y limits={0.2, upper}, title={Frame Times}, ymin=0]
        \addplot table[meta=Resolution, y=Voxelize] \dataset;
        \addplot table[meta=Resolution, y=Shadowmap] \dataset;
        \addplot table[meta=Resolution, y=Inject] \dataset;
        \addplot table[meta=Resolution, y=Filter] \dataset;
        \addplot table[meta=Resolution, y=Prepass] \dataset;
        \addplot[point meta=y, nodes near coords, nodes near coords align={above}, nodes near coords style={/pgf/number format/.cd, fixed zerofill, precision=1}] table[meta=Resolution, y=Shading] \dataset;

        \legend{Voxelize, Shadowmap, Radiance Injection, Radiance Filtering, Depth Prepass, Final Shading}
        % TODO try to get nested xticklabel
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}

% Next we compared the results of the two voxelization algorithms. Visually, they are practically indistiguishable.
\begin{frame}{Rasterized vs. Tessellated Voxels}
  \begin{figure}
      \includegraphics<1>[width=\textwidth]{results_voxelraster}
      \includegraphics<2>[width=\textwidth]{results_voxeltess}
      \caption*{\only<1>{Rasterized voxels}\only<2>{\\Tessellated voxels}}
  \end{figure}
\end{frame}

% Performance-wise the rasterization based approach was slightly faster. We might be able to close the gap with more optimizations. One benefit of the tessellated voxelization is the transformed points are not restricted to the viewport resolution. It also had a really convenient way to debug since we could add on a geometry and fragment shader to see where the vertices were. Also, it helped out with the voxel warping since we didn't have to worry about the viewport resolution (which controls the granularity of the fragment positions).
\begin{frame}{Rasterized vs. Tessellated Voxels}
  \begin{figure}
    \pgfplotstableread[row sep=\\]{
      Resolution Rasterized Tessellated\\
      64         0.53       0.70\\
      128        0.85       1.12\\
      256        1.91       2.35\\
    }\dataset
    \begin{tikzpicture}[scale=0.9]
      \begin{axis}[ybar=5pt, symbolic x coords={64,128,256}, xtick=data, legend style={at={(1.05,0.5)}, anchor=west}, xlabel={Voxel Grid Size}, ylabel={Time (ms)}, enlarge y limits={0.2, upper}, enlarge x limits={0.2}, title={Voxelization Time},nodes near coords, nodes near coords align={above}, nodes near coords style={/pgf/number format/fixed}, ymin=0]
        \addplot table[meta=Resolution, y=Rasterized] \dataset;
        \addplot table[meta=Resolution, y=Tessellated] \dataset;

        \legend{Rasterized, Tessellated}
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}

% Lastly we evaluated our voxel warping...
\begin{frame}{Voxel Warping}
\end{frame}

{\setbeamertemplate{frame footer}{*Find the source here: \url{github.com/sfreed141/vct}}
\begin{frame}{Conclusion}
  % wrap up and list contributions again
  \begin{itemize}
    \item Real-time implementation of global illumination using voxel cone tracing
    \item Implementation and comparison of two voxelization methods
    \item Investigation into warped (nonuniform) voxels
  \end{itemize}
\end{frame}}

\begin{frame}{Future Work}
  % other ways of adjusting resolution are more feasible due to temporal artifacts

  % Follow ups for voxelization and voxel warping, then general improvements

  \begin{itemize}
    \item Cascaded sparse 3D textures % uniform voxels (clearly) easier to work with. sparse textures might help alleviate memory consumption issues without having to resort to an octree
    \item Take advantage of tessellated voxelization to evenly distribute vertices for warped voxels % not restricted by rasterizer so we can put vertices where we want % voxels corresponding to view frustum still interesting idea, finding a way to reduce flickering would be great
    % \item Solid voxelization % as opposed to surface voxelization like now. rasterization vs tessellation doesn't matter much (even with optimizations most likely)
    \item Spherical harmonics, anisotropic filtering, adaptive cone tracing quality % miscellaneous improvements and optimizations
  \end{itemize}
\end{frame}

\begin{frame}[standout]
  \LARGE Thank you!\\
  \vspace{1cm}
  \LARGE Questions?
\end{frame}

% TODO bibliography slides?

\end{document}
